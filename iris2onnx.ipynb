{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa+FPK+RcMPH71dkRe57GA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tofighi/AnyLogic/blob/main/iris2onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf2onnx -q\n",
        "!pip install onnx -q"
      ],
      "metadata": {
        "id": "yMyeb-2PYMSH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import tf2onnx\n",
        "print(tensorflow.__version__)\n",
        "print (tf2onnx.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPubCu60n1Lm",
        "outputId": "667dc85b-376a-4172-a55a-fc5759a2c35f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n",
            "1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7CP5aJJYAGy",
        "outputId": "1e118415-1e9e-456b-f0c1-b343709555d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Shape:  (90, 4)\n",
            "Validation Shape:  (30, 4)\n",
            "Test Shape:  (30, 4)\n",
            "# features:  4\n",
            "# classes:  3\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_44 (Dense)            (None, 8)                 40        \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67\n",
            "Trainable params: 67\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Score: [0.5029127597808838, 0.7333333492279053]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tf2onnx.keras2onnx_api import convert_keras\n",
        "import onnx\n",
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "    \"\"\" Read and transform the training, validation and test data \"\"\"\n",
        "    iris = load_iris()\n",
        "    X = iris['data']\n",
        "    y = iris['target']\n",
        "    names = iris['target_names']\n",
        "    feature_names = iris['feature_names']\n",
        "\n",
        "    # One hot encoding\n",
        "    enc = OneHotEncoder()\n",
        "    Y = enc.fit_transform(y[:, np.newaxis]).toarray()\n",
        "\n",
        "    # Scale data to have mean 0 and variance 1\n",
        "    # which is importance for convergence of the neural network\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Split the data set into training + val and test\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=1)\n",
        "    # Split val out of \"training + val\"\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1) # 0.25*0.8=0.2\n",
        "    print(\"Train Shape: \", X_train.shape)\n",
        "    print(\"Validation Shape: \", X_val.shape)\n",
        "    print(\"Test Shape: \", X_test.shape)\n",
        "    return (X_train, Y_train), (X_val, Y_val), (X_test, Y_test)\n",
        "\n",
        "\n",
        "def create_custom_model(input_dim, output_dim, nodes, n=1, name='model'):\n",
        "    \"\"\" Build a Sequential model with the specified sepcs \"\"\"\n",
        "    # Create model\n",
        "    model = Sequential(name=name)\n",
        "    for i in range(n):\n",
        "        model.add(Dense(nodes, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(output_dim, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_pipeline(layers, nodes_per_layer):\n",
        "    \"\"\" Run thrugh the pipeline of data loading and model building/training \"\"\"\n",
        "    trains, vals, tests = load_data()\n",
        "\n",
        "    n_features = trains[0].shape[1] # x train\n",
        "    n_classes = trains[1].shape[1] # y train\n",
        "    print (\"# features: \", n_features)\n",
        "    print (\"# classes: \", n_classes)\n",
        "\n",
        "    model = create_custom_model(n_features, n_classes, nodes_per_layer, layers, 'model_1')\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    cb = EarlyStopping(monitor='loss', patience=10, min_delta=0.005)\n",
        "\n",
        "\n",
        "    verbose = False  # or False, depending on your preference\n",
        "    _ = model.fit(*trains,\n",
        "              batch_size=64,\n",
        "              epochs=200,\n",
        "              validation_data=vals,\n",
        "              callbacks=[cb],\n",
        "              verbose=1 if verbose else 0\n",
        "              )\n",
        "\n",
        "    score = model.evaluate(*tests, verbose=0)\n",
        "    return model, score\n",
        "\n",
        "\n",
        "def save_to_onnx(model, filename):\n",
        "    \"\"\" Save a keras model as an ONNX file \"\"\"\n",
        "    proto = convert_keras(model)\n",
        "\n",
        "    if not filename.endswith(\".onnx\"): filename += \".onnx\"\n",
        "    onnx.save(proto, filename)\n",
        "\n",
        "#Run and Save Model\n",
        "model, score = train_pipeline(1, 8)\n",
        "print(\"Score:\", score)\n",
        "\n",
        "save_to_onnx(model, \"iris.onnx\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YunwZYqErmQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}